# ============================================================================
# COMPARATIVA DE M√âTODOS: PCA, K-Means y Clustering Jer√°rquico
# Dataset: Wine
# Incluye varianza explicada, m√©tricas de calidad y tiempo de ejecuci√≥n
# ============================================================================

from sklearn.datasets import load_wine
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from scipy.cluster.hierarchy import linkage, fcluster
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import time

# ----------------------------------------------------------------------------
# 1. CARGA Y ESCALADO DE DATOS
# ----------------------------------------------------------------------------
wine = load_wine()
X = wine.data
y_true = wine.target
feature_names = wine.feature_names

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# ----------------------------------------------------------------------------
# 2. PCA - Reducci√≥n de Dimensionalidad
# ----------------------------------------------------------------------------
print("\nEjecutando PCA...")
start = time.time()
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)
time_pca = time.time() - start
var_exp = pca.explained_variance_ratio_.sum()

print(f"PCA completado en {time_pca:.4f} s (varianza explicada: {var_exp:.2%})")

# ----------------------------------------------------------------------------
# 3. K-MEANS CLUSTERING
# ----------------------------------------------------------------------------
print("\nEjecutando K-Means...")
start = time.time()
kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)
y_kmeans = kmeans.fit_predict(X_scaled)
time_kmeans = time.time() - start

sil_k = silhouette_score(X_scaled, y_kmeans)
db_k = davies_bouldin_score(X_scaled, y_kmeans)
ch_k = calinski_harabasz_score(X_scaled, y_kmeans)

print(f"K-Means completado en {time_kmeans:.4f} s")

# ----------------------------------------------------------------------------
# 4. CLUSTERING JER√ÅRQUICO
# ----------------------------------------------------------------------------
print("\nEjecutando Clustering Jer√°rquico (Ward)...")
start = time.time()
Z = linkage(X_scaled, method='ward')
y_hier = fcluster(Z, t=3, criterion='maxclust')
time_hier = time.time() - start

sil_h = silhouette_score(X_scaled, y_hier)
db_h = davies_bouldin_score(X_scaled, y_hier)
ch_h = calinski_harabasz_score(X_scaled, y_hier)

print(f"Jer√°rquico completado en {time_hier:.4f} s")

# ----------------------------------------------------------------------------
# 5. TABLA DE RESULTADOS Y RANKING
# ----------------------------------------------------------------------------
resultados = pd.DataFrame({
    'M√©todo': ['PCA (referencia)', 'K-Means', 'Jer√°rquico'],
    'Silhouette': [np.nan, sil_k, sil_h],
    'Davies-Bouldin (‚Üì)': [np.nan, db_k, db_h],
    'Calinski-Harabasz (‚Üë)': [np.nan, ch_k, ch_h],
    'Varianza Explicada (PCA)': [var_exp, np.nan, np.nan],
    'Tiempo (s)': [time_pca, time_kmeans, time_hier]
})

# Ranking solo entre m√©todos de clustering
resultados['Ranking'] = (
    resultados['Silhouette'].rank(ascending=False) +
    resultados['Calinski-Harabasz (‚Üë)'].rank(ascending=False) +
    resultados['Davies-Bouldin (‚Üì)'].rank(ascending=True)
)

resultados = resultados.sort_values('Ranking', na_position='first').reset_index(drop=True)

print("\n" + "=" * 80)
print("RESULTADOS COMPARATIVOS")
print("=" * 80)
print(resultados.to_string(index=False, float_format=lambda x: f"{x:.3f}" if isinstance(x, float) else str(x)))

# ----------------------------------------------------------------------------
# 6. VISUALIZACI√ìN PCA + CLUSTERS
# ----------------------------------------------------------------------------
fig, axs = plt.subplots(1, 2, figsize=(12, 5))
colores = ['red', 'green', 'blue']

# K-Means
axs[0].set_title("K-Means sobre componentes PCA")
for i in range(3):
    mask = y_kmeans == i
    axs[0].scatter(X_pca[mask, 0], X_pca[mask, 1],
                   label=f'Cluster {i}', s=80, alpha=0.6, edgecolors='black', color=colores[i])
axs[0].legend()
axs[0].grid(True, alpha=0.3)
axs[0].set_xlabel("PC1")
axs[0].set_ylabel("PC2")

# Jer√°rquico
axs[1].set_title("Jer√°rquico (Ward) sobre componentes PCA")
for i in range(3):
    mask = y_hier == i + 1
    axs[1].scatter(X_pca[mask, 0], X_pca[mask, 1],
                   label=f'Cluster {i+1}', s=80, alpha=0.6, edgecolors='black', color=colores[i])
axs[1].legend()
axs[1].grid(True, alpha=0.3)
axs[1].set_xlabel("PC1")
axs[1].set_ylabel("PC2")

plt.tight_layout()
plt.show()

# ----------------------------------------------------------------------------
# 7. INTERPRETACI√ìN FINAL
# ----------------------------------------------------------------------------
print("\n" + "=" * 80)
print("INTERPRETACI√ìN Y CONCLUSI√ìN")
print("=" * 80)
print(f"\nüìä PCA explic√≥ el {var_exp:.2%} de la varianza total con 2 componentes "
      f"en {time_pca:.4f} segundos.")
print("Sirve como referencia visual o paso previo al clustering.")

if resultados.iloc[-1]['M√©todo'] == 'K-Means':
    print("\nüèÜ El mejor m√©todo de agrupamiento fue **K-Means**: "
          "produce clusters compactos y es el m√°s r√°pido.")
elif resultados.iloc[-1]['M√©todo'] == 'Jer√°rquico':
    print("\nüèÜ El mejor m√©todo de agrupamiento fue **Jer√°rquico**: "
          "revela una estructura jer√°rquica m√°s rica aunque tarda m√°s.")
else:
    print("\n‚ÑπÔ∏è PCA no se usa para clustering, pero ayuda a visualizar los grupos obtenidos.")

print("\nüìå Recomendaciones:")
print("- Usa **PCA** para visualizar o reducir la dimensionalidad antes de agrupar.")
print("- Usa **K-Means** si sospechas que los grupos son compactos y similares.")
print("- Usa **Jer√°rquico** si quieres explorar niveles o subgrupos sin fijar K.")
print("- Considera **tiempo de ejecuci√≥n**: PCA y K-Means son mucho m√°s r√°pidos que Ward.")
