# ============================================================================
# COMPARATIVA DE M√âTODOS: PCA, K-Means y Clustering Jer√°rquico
# Dataset: Wine
# ============================================================================

from sklearn.datasets import load_wine
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from scipy.cluster.hierarchy import linkage, fcluster
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# ----------------------------------------------------------------------------
# 1. CARGAR Y PREPARAR DATOS
# ----------------------------------------------------------------------------
wine = load_wine()
X = wine.data
y_true = wine.target
feature_names = wine.feature_names

X_scaled = StandardScaler().fit_transform(X)

# ----------------------------------------------------------------------------
# 2. PCA (Reducci√≥n de dimensionalidad)
# ----------------------------------------------------------------------------
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)
var_exp = pca.explained_variance_ratio_.sum()

# ----------------------------------------------------------------------------
# 3. K-MEANS
# ----------------------------------------------------------------------------
kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)
y_kmeans = kmeans.fit_predict(X_scaled)

sil_k = silhouette_score(X_scaled, y_kmeans)
db_k = davies_bouldin_score(X_scaled, y_kmeans)
ch_k = calinski_harabasz_score(X_scaled, y_kmeans)

# ----------------------------------------------------------------------------
# 4. CLUSTERING JER√ÅRQUICO
# ----------------------------------------------------------------------------
Z = linkage(X_scaled, method='ward')
y_hier = fcluster(Z, t=3, criterion='maxclust')

sil_h = silhouette_score(X_scaled, y_hier)
db_h = davies_bouldin_score(X_scaled, y_hier)
ch_h = calinski_harabasz_score(X_scaled, y_hier)

# ----------------------------------------------------------------------------
# 5. RESULTADOS Y RANKING
# ----------------------------------------------------------------------------
resultados = pd.DataFrame({
    'M√©todo': ['K-Means', 'Jer√°rquico'],
    'Silhouette': [sil_k, sil_h],
    'Davies-Bouldin (‚Üì)': [db_k, db_h],
    'Calinski-Harabasz (‚Üë)': [ch_k, ch_h]
})

# Ranking de calidad: m√°s alto Silhouette + Calinski, menor Davies
resultados['Ranking'] = (
    resultados['Silhouette'].rank(ascending=False) +
    resultados['Calinski-Harabasz (‚Üë)'].rank(ascending=False) +
    resultados['Davies-Bouldin (‚Üì)'].rank(ascending=True)
)

resultados = resultados.sort_values('Ranking').reset_index(drop=True)

print("\n" + "=" * 70)
print("RESULTADOS COMPARATIVOS")
print("=" * 70)
print(resultados.to_string(index=False))

# ----------------------------------------------------------------------------
# 6. VISUALIZACI√ìN COMPARATIVA
# ----------------------------------------------------------------------------
plt.figure(figsize=(8, 5))
plt.bar(resultados['M√©todo'], resultados['Silhouette'], color=['skyblue', 'lightcoral'])
plt.title('Comparativa de M√©todos de Clustering (Silhouette Score)')
plt.ylabel('Silhouette Score')
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()

# ----------------------------------------------------------------------------
# 7. INTERPRETACI√ìN Y CONCLUSI√ìN
# ----------------------------------------------------------------------------
print("\n" + "=" * 70)
print("INTERPRETACI√ìN Y CONCLUSI√ìN")
print("=" * 70)

print(f"\nüìä PCA explic√≥ el {var_exp:.2%} de la varianza total con 2 componentes.")
print("Esto permite visualizar los clusters pero no agruparlos por s√≠ solo.")

if resultados.iloc[0]['M√©todo'] == 'K-Means':
    print("\nüèÜ El mejor m√©todo de agrupamiento fue **K-Means**, "
          "lo que indica que los vinos forman clusters m√°s compactos y bien separados.")
    print("Es ideal cuando se sospecha que los grupos son esf√©ricos y de tama√±o similar.")
else:
    print("\nüèÜ El mejor m√©todo de agrupamiento fue **Jer√°rquico**, "
          "lo que indica que los vinos tienen una estructura jer√°rquica m√°s rica.")
    print("Es √∫til para descubrir subgrupos o relaciones progresivas entre clases.")

print("\nüìå En general:")
print("- Usa **PCA** para explorar o reducir variables antes de agrupar.")
print("- Usa **K-Means** si conoces el n√∫mero aproximado de clusters y tus datos son bien definidos.")
print("- Usa **Jer√°rquico** si quieres descubrir niveles de similitud o no sabes cu√°ntos clusters hay.")
